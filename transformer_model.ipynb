{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ricar\\anaconda3\\envs\\pro24\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Embedding, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('aggregated_data4.csv')\n",
    "\n",
    "# Convert 'YearMonth' to datetime and extract features\n",
    "df['YearMonth'] = pd.to_datetime(df['YearMonth'])\n",
    "df['Year'] = df['YearMonth'].dt.year\n",
    "df['Month'] = df['YearMonth'].dt.month\n",
    "df.drop(['YearMonth'], axis=1, inplace=True)\n",
    "\n",
    "# Separate target and features\n",
    "y = df.pop('PESO DESEMBARCADO_KILOGRAMOS')\n",
    "X = df\n",
    "\n",
    "# Define the column transformer\n",
    "numeric_features = ['SST', 'Year', 'Month']\n",
    "numeric_transformer = StandardScaler()\n",
    "\n",
    "categorical_features = ['NOMBRE PRINCIPAL', 'NOMBRE ESTADO', 'NOMBRE OFICINA']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Fit and transform the training data\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Split the processed data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ricar\\anaconda3\\envs\\pro24\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Exception encountered when calling layer 'softmax' (type Softmax).\n\ntuple index out of range\n\nCall arguments received by layer 'softmax' (type Softmax):\n  • inputs=tf.Tensor(shape=(None, 4), dtype=float32)\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# This should now work since X_train is preprocessed\u001b[39;00m\n\u001b[0;32m     15\u001b[0m inputs \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mff_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m Flatten()(x)\n\u001b[0;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mtransformer_encoder\u001b[1;34m(inputs, head_size, num_heads, ff_dim, dropout)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransformer_encoder\u001b[39m(inputs, head_size, num_heads, ff_dim, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m      2\u001b[0m     x \u001b[38;5;241m=\u001b[39m LayerNormalization(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)(inputs)\n\u001b[1;32m----> 3\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     x \u001b[38;5;241m=\u001b[39m Dropout(dropout)(x)\n\u001b[0;32m      5\u001b[0m     res \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m inputs\n",
      "File \u001b[1;32mc:\\Users\\ricar\\anaconda3\\envs\\pro24\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\ricar\\anaconda3\\envs\\pro24\\Lib\\site-packages\\keras\\src\\layers\\activation\\softmax.py:107\u001b[0m, in \u001b[0;36mSoftmax.call\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexp(\n\u001b[0;32m    103\u001b[0m             inputs\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_logsumexp(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    105\u001b[0m         )\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msoftmax(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msoftmax(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis)\n",
      "\u001b[1;31mIndexError\u001b[0m: Exception encountered when calling layer 'softmax' (type Softmax).\n\ntuple index out of range\n\nCall arguments received by layer 'softmax' (type Softmax):\n  • inputs=tf.Tensor(shape=(None, 4), dtype=float32)\n  • mask=None"
     ]
    }
   ],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    return x + res\n",
    "\n",
    "# Assuming X_train is now a numpy array after preprocessing\n",
    "input_shape = X_train.shape[1:]  # This should now work since X_train is preprocessed\n",
    "inputs = Input(shape=input_shape)\n",
    "x = transformer_encoder(inputs, head_size=256, num_heads=4, ff_dim=4, dropout=0.25)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "model = Model(inputs, x)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4), loss='mean_squared_error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "preds = model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, preds)\n",
    "print(f'MSE: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pro24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
