{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Concatenate, Lambda, Dropout, Embedding, Multiply\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon())\n",
    "\n",
    "class MoEWeightingLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_experts, **kwargs):\n",
    "        super(MoEWeightingLayer, self).__init__(**kwargs)\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name='weights', shape=(input_shape[1], self.num_experts), initializer='uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w)\n",
    "\n",
    "def build_moe_model(num_offices, num_species, input_shape, lstm_units=50, dropout_rate=0.1):\n",
    "    main_input = Input(shape=input_shape)\n",
    "    office_input = Input(shape=(1,))\n",
    "    species_input = Input(shape=(1,))\n",
    "    \n",
    "    expert_outputs = []\n",
    "    for _ in range(num_offices):\n",
    "        x = LSTM(lstm_units, return_sequences=True)(main_input)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = LSTM(lstm_units)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        expert_outputs.append(Dense(1)(x))\n",
    "\n",
    "    concatenated = Concatenate()(expert_outputs)\n",
    "    office_weights_output = Dense(num_offices, activation='softmax')(concatenated)\n",
    "    office_output = Lambda(lambda x: tf.reduce_sum(x, axis=1))(office_weights_output)\n",
    "\n",
    "    species_embedding = Embedding(num_species, 1, input_length=1)(species_input)\n",
    "    species_embedding = Lambda(lambda x: tf.squeeze(x, axis=-1))(species_embedding)\n",
    "    adjusted_output = Multiply()([office_output, species_embedding])\n",
    "\n",
    "    model = Model(inputs=[main_input, office_input, species_input], outputs=adjusted_output)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse', r_squared])\n",
    "    return model\n",
    "\n",
    "def prepare_data_and_train_model(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    office_encoder = LabelEncoder()\n",
    "    species_encoder = LabelEncoder()\n",
    "    df['NOMBRE_OFICINA_encoded'] = office_encoder.fit_transform(df['NOMBRE OFICINA'])\n",
    "    df['NOMBRE_PRINCIPAL_encoded'] = species_encoder.fit_transform(df['NOMBRE PRINCIPAL'])\n",
    "    \n",
    "    scaler_sst = MinMaxScaler()\n",
    "    scaler_weight = MinMaxScaler()\n",
    "    df['SST_scaled'] = scaler_sst.fit_transform(df[['SST']])\n",
    "    df['PESO_DESEMBARCADO_scaled'] = scaler_weight.fit_transform(df[['PESO DESEMBARCADO_KILOGRAMOS']])\n",
    "    \n",
    "    X = np.array(df[['SST_scaled']])\n",
    "    y = np.array(df['PESO_DESEMBARCADO_scaled'])\n",
    "    offices = np.array(df['NOMBRE_OFICINA_encoded'])\n",
    "    species = np.array(df['NOMBRE_PRINCIPAL_encoded'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test, offices_train, offices_test, species_train, species_test = train_test_split(\n",
    "        X, y, offices, species, test_size=0.2, random_state=42)\n",
    "\n",
    "    num_offices = df['NOMBRE_OFICINA_encoded'].nunique()\n",
    "    num_species = df['NOMBRE_PRINCIPAL_encoded'].nunique()\n",
    "    \n",
    "    model = build_moe_model(num_offices, num_species, (X_train.shape[1], 1))\n",
    "    \n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, mode='min', restore_best_weights=True)\n",
    "    model.fit([X_train, offices_train, species_train], y_train, epochs=50, batch_size=32, validation_split=0.2,\n",
    "              callbacks=[early_stop, tensorboard_callback], verbose=1)\n",
    "\n",
    "    mse = model.evaluate([X_test, offices_test, species_test], y_test, verbose=0)\n",
    "    print(f'Test MSE: {mse}')\n",
    "    \n",
    "    return model, office_encoder, species_encoder, scaler_sst, scaler_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use the returned objects\n",
    "model, office_encoder, species_encoder, scaler_sst, scaler_weight = prepare_data_and_train_model('aggregated_data4.csv')\n",
    "\n",
    "# Saving the encoders and scaler\n",
    "with open('office_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(office_encoder, file)\n",
    "with open('species_encoder.pkl', 'wb') as file:\n",
    "    pickle.dump(species_encoder, file)\n",
    "with open('sst_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_sst, file)\n",
    "with open('weight_scaler.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler_weight, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Function to prepare new data using the loaded encoders and scaler\n",
    "def prepare_new_data(new_data, office_encoder, species_encoder, scaler):\n",
    "    # Assume new_data is a DataFrame with columns 'SST', 'NOMBRE_OFICINA', 'NOMBRE_PRINCIPAL'\n",
    "    new_data['NOMBRE_OFICINA_encoded'] = office_encoder.transform(new_data['NOMBRE_OFICINA'])\n",
    "    new_data['NOMBRE_PRINCIPAL_encoded'] = species_encoder.transform(new_data['NOMBRE_PRINCIPAL'])\n",
    "    \n",
    "    # Applying the MinMax scaling to the SST column\n",
    "    new_data['SST_scaled'] = scaler.transform(new_data[['SST']])\n",
    "    \n",
    "    X = np.array(new_data['SST_scaled'])\n",
    "    offices = np.array(new_data['NOMBRE_OFICINA_encoded'])\n",
    "    species = np.array(new_data['NOMBRE_PRINCIPAL_encoded'])\n",
    "    \n",
    "    # Assuming that the input_shape[1] (feature dimension) is 1\n",
    "    input_shape = (X.shape[1], 1)\n",
    "    return [X.reshape(-1, input_shape[0], input_shape[1]), offices, species]\n",
    "\n",
    "# Example usage:\n",
    "# new_data = pd.DataFrame({\n",
    "#     'SST': [25.5],  # Example sea surface temperature\n",
    "#     'NOMBRE_OFICINA': ['Office1'],  # Example office name\n",
    "#     'NOMBRE_PRINCIPAL': ['Species1']  # Example species name\n",
    "# })\n",
    "\n",
    "# Load the scaler from disk\n",
    "with open('sst_scaler.pkl', 'rb') as file:\n",
    "    sst_scaler = pickle.load(file)\n",
    "\n",
    "# Prepare the new data using the loaded encoders and scaler\n",
    "prepared_data = prepare_new_data(new_data, office_encoder, species_encoder, sst_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to use the model to predict with new data\n",
    "def predict_with_new_data(model, prepared_data):\n",
    "    predictions = model.predict(prepared_data)\n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "new_data = pd.DataFrame({\n",
    "    'SST': [25.5],  # Example sea surface temperature\n",
    "    'NOMBRE_OFICINA': ['Office1'],  # Example office name\n",
    "    'NOMBRE_PRINCIPAL': ['Species1']  # Example species name\n",
    "})\n",
    "\n",
    "prepared_data = prepare_new_data(new_data, office_encoder, species_encoder)\n",
    "predictions = predict_with_new_data(model, prepared_data)\n",
    "print(\"Predicted Catch:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
