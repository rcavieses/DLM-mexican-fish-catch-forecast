{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 08:42:21.961933: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-04 08:42:21.964305: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-04 08:42:21.996397: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-04 08:42:21.996416: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-04 08:42:21.997292: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-04 08:42:22.002421: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-04 08:42:22.003034: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-04 08:42:22.677825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Reshape, Concatenate, Multiply, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon())\n",
    "\n",
    "def build_expert(input_shape, dropout_rate, name):\n",
    "    # Starting with an Input layer\n",
    "    input_layer = Input(shape=input_shape, name=f\"input_{name}\")\n",
    "    # First Dense layer to transform the input\n",
    "    dense = Dense(15, activation='relu', name=f\"dense_start_{name}\")(input_layer)\n",
    "    # First LSTM layer\n",
    "    lstm1 = LSTM(15, return_sequences=True, name=f\"lstm1_{name}\")(dense)\n",
    "    dropout1 = Dropout(dropout_rate, name=f\"dropout1_{name}\")(lstm1)\n",
    "    # Second LSTM layer\n",
    "    lstm2 = LSTM(15, return_sequences=False, name=f\"lstm2_{name}\")(dropout1)\n",
    "    dropout2 = Dropout(dropout_rate, name=f\"dropout2_{name}\")(lstm2)\n",
    "    # Final Dense layer for output\n",
    "    output_layer = Dense(1, activation='linear', name=f\"dense_output_{name}\")(dropout2)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=output_layer, name=f\"expert_{name}\")\n",
    "\n",
    "def build_moe_model(num_offices, num_species, input_shape, dropout_rate=0.1):\n",
    "    # Main input for the model\n",
    "    main_input = Input(shape=input_shape, name=\"main_input\")\n",
    "    \n",
    "    # Inputs for office and species, which will determine expert selection\n",
    "    office_input = Input(shape=(1,), dtype='int32', name=\"office_input\")\n",
    "    species_input = Input(shape=(1,), dtype='int32', name=\"species_input\")\n",
    "    \n",
    "    # Embeddings for office and species\n",
    "    office_embedding = Embedding(num_offices, num_offices * num_species, input_length=1, name=\"office_embedding\")(office_input)\n",
    "    species_embedding = Embedding(num_species, num_offices * num_species, input_length=1, name=\"species_embedding\")(species_input)\n",
    "    \n",
    "    # Flatten embeddings\n",
    "    office_flat = Reshape((num_offices * num_species,))(office_embedding)\n",
    "    species_flat = Reshape((num_offices * num_species,))(species_embedding)\n",
    "    \n",
    "    # Element-wise multiplication to combine embeddings, acting as gating mechanism\n",
    "    combined_gates = Multiply(name=\"multiply_gates\")([office_flat, species_flat])\n",
    "    \n",
    "    # Build experts\n",
    "    experts = [build_expert(input_shape, dropout_rate, f\"office_{o}_species_{s}\")\n",
    "               for o in range(num_offices) for s in range(num_species)]\n",
    "    \n",
    "    # Expert outputs\n",
    "    expert_outputs = [expert(main_input) for expert in experts]\n",
    "    \n",
    "    # Concatenate all expert outputs\n",
    "    concatenated_outputs = Concatenate(name=\"concatenate_experts\")(expert_outputs)\n",
    "    \n",
    "    # Weighted sum of expert outputs based on combined gates\n",
    "    final_output = Multiply(name=\"weighted_sum\")([concatenated_outputs, combined_gates])\n",
    "    \n",
    "    # Final model\n",
    "    model = Model(inputs=[main_input, office_input, species_input], outputs=final_output)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse',r_squared,tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "def prepare_data_and_train_model(filename, dropout_rate=0.1):\n",
    "    df = pd.read_csv(filename)\n",
    "    office_encoder = LabelEncoder()\n",
    "    species_encoder = LabelEncoder()\n",
    "    df['NOMBRE_OFICINA_encoded'] = office_encoder.fit_transform(df['NOMBRE OFICINA'])\n",
    "    df['NOMBRE_PRINCIPAL_encoded'] = species_encoder.fit_transform(df['NOMBRE PRINCIPAL'])\n",
    "    \n",
    "    scaler_sst = MinMaxScaler()\n",
    "    scaler_weight = MinMaxScaler()\n",
    "    df['SST_scaled'] = scaler_sst.fit_transform(df[['SST']])\n",
    "    df['PESO_DESEMBARCADO_scaled'] = scaler_weight.fit_transform(df[['PESO DESEMBARCADO_KILOGRAMOS']])\n",
    "    \n",
    "    X = np.array(df[['SST_scaled']])\n",
    "    y = np.array(df['PESO_DESEMBARCADO_scaled'])\n",
    "    offices = np.array(df['NOMBRE_OFICINA_encoded'])\n",
    "    species = np.array(df['NOMBRE_PRINCIPAL_encoded'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test, offices_train, offices_test, species_train, species_test = train_test_split(\n",
    "        X, y, offices, species, test_size=0.2, random_state=42)\n",
    "\n",
    "    num_offices = df['NOMBRE_OFICINA_encoded'].nunique()\n",
    "    num_species = df['NOMBRE_PRINCIPAL_encoded'].nunique()\n",
    "    num_experts = num_offices * num_species\n",
    "    print(f\"Number of experts to be trained: {num_experts}\")\n",
    "    # Proper reshaping for LSTM input\n",
    "    X_train = X_train.reshape(-1, 1, 1)  # Reshape to (samples, timesteps, features)\n",
    "    X_test = X_test.reshape(-1, 1, 1)\n",
    "\n",
    "    # Create model\n",
    "    model = build_moe_model(num_offices, num_species, (1, 1), dropout_rate)\n",
    "    \n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, mode='min', restore_best_weights=True)\n",
    "    model.fit([X_train, offices_train, species_train], y_train, epochs=50, batch_size=64, validation_split=0.2,\n",
    "              callbacks=[early_stop, tensorboard_callback], verbose=1)\n",
    "\n",
    "    mse = model.evaluate([X_test, offices_test, species_test], y_test, verbose=0)\n",
    "    print(f'Test MSE: {mse}')\n",
    "    \n",
    "    return model, office_encoder, species_encoder, scaler_sst, scaler_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of experts to be trained: 500\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the function to prepare data, build the model, and train it\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model, office_encoder, species_encoder, scaler_sst, scaler_weight \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_and_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 117\u001b[0m, in \u001b[0;36mprepare_data_and_train_model\u001b[0;34m(filename, dropout_rate)\u001b[0m\n\u001b[1;32m    114\u001b[0m tensorboard_callback \u001b[38;5;241m=\u001b[39m TensorBoard(log_dir\u001b[38;5;241m=\u001b[39mlog_dir, histogram_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, write_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 117\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffices_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m mse \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate([X_test, offices_test, species_test], y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/tmp/__autograph_generated_filex2055d_r.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1155, in train_step\n        return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1249, in compute_metrics\n        self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 620, in update_state\n        metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/utils/metrics_utils.py\", line 77, in decorated\n        result = update_state_fn(*args, **kwargs)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 140, in update_state_fn\n        return ag_update_state(*args, **kwargs)\n    File \"/opt/conda/envs/cedo/lib/python3.11/site-packages/keras/src/metrics/base_metric.py\", line 723, in update_state  **\n        matches = ag_fn(y_true, y_pred, **self._fn_kwargs)\n\n    TypeError: 'str' object is not callable\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your CSV file\n",
    "filename = 'filtered_data.csv'\n",
    "\n",
    "# Specify the dropout rate for the LSTM layers\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Call the function to prepare data, build the model, and train it\n",
    "model, office_encoder, species_encoder, scaler_sst, scaler_weight = prepare_data_and_train_model(\n",
    "    filename, \n",
    "    dropout_rate\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
