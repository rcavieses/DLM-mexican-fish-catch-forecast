{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding, Reshape, Concatenate, Multiply\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    return 1 - ss_res / (ss_tot + tf.keras.backend.epsilon())\n",
    "\n",
    "def build_expert(input_shape, num_neurons, dropout_rate, name):\n",
    "    # Simple LSTM expert model\n",
    "    input_layer = Input(shape=input_shape, name=f\"input_{name}\")\n",
    "    lstm = LSTM(num_neurons, return_sequences=False, name=f\"lstm_{name}\")(input_layer)\n",
    "    dropout = tf.keras.layers.Dropout(dropout_rate, name=f\"dropout_{name}\")(lstm)\n",
    "    output_layer = Dense(1, activation='linear', name=f\"dense_{name}\")(dropout)\n",
    "    return Model(inputs=input_layer, outputs=output_layer, name=f\"expert_{name}\")\n",
    "\n",
    "def build_moe_model(num_offices, num_species, num_neurons_per_expert, input_shape, dropout_rate=0.1):\n",
    "    # Main input for the model\n",
    "    main_input = Input(shape=input_shape, name=\"main_input\")\n",
    "    \n",
    "    # Inputs for office and species, which will determine expert selection\n",
    "    office_input = Input(shape=(1,), dtype='int32', name=\"office_input\")\n",
    "    species_input = Input(shape=(1,), dtype='int32', name=\"species_input\")\n",
    "    \n",
    "    # Embeddings for office and species\n",
    "    office_embedding = Embedding(num_offices, num_offices * num_species, input_length=1, name=\"office_embedding\")(office_input)\n",
    "    species_embedding = Embedding(num_species, num_offices * num_species, input_length=1, name=\"species_embedding\")(species_input)\n",
    "    \n",
    "    # Flatten embeddings\n",
    "    office_flat = Reshape((num_offices * num_species,))(office_embedding)\n",
    "    species_flat = Reshape((num_offices * num_species,))(species_embedding)\n",
    "    \n",
    "    # Element-wise multiplication to combine embeddings, acting as gating mechanism\n",
    "    combined_gates = Multiply(name=\"multiply_gates\")([office_flat, species_flat])\n",
    "    \n",
    "    # Build experts\n",
    "    experts = [build_expert(input_shape, num_neurons_per_expert, dropout_rate, f\"office_{o}_species_{s}\")\n",
    "               for o in range(num_offices) for s in range(num_species)]\n",
    "    \n",
    "    # Expert outputs\n",
    "    expert_outputs = [expert(main_input) for expert in experts]\n",
    "    \n",
    "    # Concatenate all expert outputs\n",
    "    concatenated_outputs = Concatenate(name=\"concatenate_experts\")(expert_outputs)\n",
    "    \n",
    "    # Weighted sum of expert outputs based on combined gates\n",
    "    final_output = Multiply(name=\"weighted_sum\")([concatenated_outputs, combined_gates])\n",
    "    \n",
    "    # Final model\n",
    "    model = Model(inputs=[main_input, office_input, species_input], outputs=final_output)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse',r_squared,tf.keras.metrics.RootMeanSquaredError()])\n",
    "\n",
    "    return model\n",
    "\n",
    "def prepare_data_and_train_model(filename, num_neurons_per_expert, dropout_rate=0.1):\n",
    "    df = pd.read_csv(filename)\n",
    "    office_encoder = LabelEncoder()\n",
    "    species_encoder = LabelEncoder()\n",
    "    df['NOMBRE_OFICINA_encoded'] = office_encoder.fit_transform(df['NOMBRE OFICINA'])\n",
    "    df['NOMBRE_PRINCIPAL_encoded'] = species_encoder.fit_transform(df['NOMBRE PRINCIPAL'])\n",
    "    \n",
    "    scaler_sst = MinMaxScaler()\n",
    "    scaler_weight = MinMaxScaler()\n",
    "    df['SST_scaled'] = scaler_sst.fit_transform(df[['SST']])\n",
    "    df['PESO_DESEMBARCADO_scaled'] = scaler_weight.fit_transform(df[['PESO DESEMBARCADO_KILOGRAMOS']])\n",
    "    \n",
    "    X = np.array(df[['SST_scaled']])\n",
    "    y = np.array(df['PESO_DESEMBARCADO_scaled'])\n",
    "    offices = np.array(df['NOMBRE_OFICINA_encoded'])\n",
    "    species = np.array(df['NOMBRE_PRINCIPAL_encoded'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test, offices_train, offices_test, species_train, species_test = train_test_split(\n",
    "        X, y, offices, species, test_size=0.2, random_state=42)\n",
    "\n",
    "    num_offices = df['NOMBRE_OFICINA_encoded'].nunique()\n",
    "    num_species = df['NOMBRE_PRINCIPAL_encoded'].nunique()\n",
    "    \n",
    "    # Proper reshaping for LSTM input\n",
    "    X_train = X_train.reshape(-1, 1, 1)  # Reshape to (samples, timesteps, features)\n",
    "    X_test = X_test.reshape(-1, 1, 1)\n",
    "\n",
    "    # Create model\n",
    "    model = build_moe_model(num_offices, num_species, num_neurons_per_expert, (1, 1), dropout_rate)\n",
    "    \n",
    "    log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, mode='min', restore_best_weights=True)\n",
    "    model.fit([X_train, offices_train, species_train], y_train, epochs=50, batch_size=64, validation_split=0.2,\n",
    "              callbacks=[early_stop, tensorboard_callback], verbose=1)\n",
    "\n",
    "    mse = model.evaluate([X_test, offices_test, species_test], y_test, verbose=0)\n",
    "    print(f'Test MSE: {mse}')\n",
    "    \n",
    "    return model, office_encoder, species_encoder, scaler_sst, scaler_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Call the function to prepare data, build the model, and train it\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model, office_encoder, species_encoder, scaler_sst, scaler_weight \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_and_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_neurons_per_expert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 90\u001b[0m, in \u001b[0;36mprepare_data_and_train_model\u001b[0;34m(filename, num_neurons_per_expert, dropout_rate)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Proper reshaping for LSTM input\u001b[39;00m\n\u001b[1;32m     89\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape to (samples, timesteps, features)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[43mir\u001b[49m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Create model\u001b[39;00m\n\u001b[1;32m     93\u001b[0m model \u001b[38;5;241m=\u001b[39m build_moe_model(num_offices, num_species, num_neurons_per_expert, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dropout_rate)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ir' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify the path to your CSV file\n",
    "filename = 'aggregated_data4.csv'\n",
    "\n",
    "# Specify the number of neurons per expert and the dropout rate for the LSTM layers\n",
    "num_neurons_per_expert = 10\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Call the function to prepare data, build the model, and train it\n",
    "model, office_encoder, species_encoder, scaler_sst, scaler_weight = prepare_data_and_train_model(\n",
    "    filename, \n",
    "    num_neurons_per_expert, \n",
    "    dropout_rate\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cedo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
